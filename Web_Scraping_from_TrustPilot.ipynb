from time import sleep
import requests
import pandas as pd
from bs4 import BeautifulSoup
import json


def soup2list(src, list_, attr=None):
    if attr:
        for val in src:
            list_.append(val[attr])
    else:
        for val in src:
            list_.append(val.get_text())

users = []
userReviewNum = []
ratings = []
locations = []
dates = []
reviews = []

from_page = 1
to_page = 10
company = 'capcut.com'

reviews_found = []

for i in range(from_page, to_page + 1):
    url = f"https://www.trustpilot.com/review/{company}?page={i}"
    try:
        result = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    except Exception as e:
        print(f"Failed to fetch page {i}: {e}")
        continue

    print("Page:", i, "Status:", result.status_code)
    print("First 200 chars:", result.text[:200])

    soup = BeautifulSoup(result.content, 'html.parser')


    # TrustPilot isn't very scraper-friendly; use explicit class dicts (not sets)
    next_data = soup.find("script", id="__NEXT_DATA__")
    if not next_data:
        print("No __NEXT_DATA__ found on page", i)
        continue

    data = json.loads(next_data.string)


    def find_reviews(obj):
        if isinstance(obj, dict):
            # heuristic: review-like objects
            if "rating" in obj and "text" in obj:
                reviews_found.append(obj)
            for v in obj.values():
                find_reviews(v)
        elif isinstance(obj, list):
            for item in obj:
                find_reviews(item)

    find_reviews(data)

    print("Review-like objects found:", len(reviews_found))


